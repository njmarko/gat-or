{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gat-or.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njmarko/gat-or/blob/master/gat_or.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GATor\n",
        "\n",
        "GATor is a Graph Attention Network for object detection with relational reasoning."
      ],
      "metadata": {
        "id": "1PQbehGvj_kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing dependencies and downloading dataset"
      ],
      "metadata": {
        "id": "BtzUQf4vkMsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!pip uninstall opencv-python-headless -y\n",
        "!pip install opencv-python-headless==4.1.2.30\n",
        "!pip install fiftyone\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "UKwSnuW4L-jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "\n",
        "cd ..\n",
        "rm -r vision"
      ],
      "metadata": {
        "id": "IVAIYHqQkX9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.utils.coco as fouc\n",
        "from fiftyone import ViewField as VF\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as TVF\n",
        "from torchvision.ops import box_iou\n",
        "from transforms import Compose, ToTensor\n",
        "from PIL import Image\n",
        "import matplotlib.pylab as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.pyplot import figure\n",
        "import torch_geometric as pyg\n",
        "import torch_geometric.data as pyg_data\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils"
      ],
      "metadata": {
        "id": "XN38W3_Wkccv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"train\",\n",
        "    max_samples=200\n",
        ")\n",
        "dataset.persistent = True\n",
        "\n",
        "session = fo.launch_app(dataset)"
      ],
      "metadata": {
        "id": "PggQ7R73mOk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing co-occurrence matrix"
      ],
      "metadata": {
        "id": "8fNrbUQOm5Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['background','person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light','fire hydrant','street sign','stop sign','parking meter','bench','bird','cat','dog','horse',\n",
        "'sheep','cow','elephant','bear','zebra','giraffe','hat','backpack','umbrella','shoe','eye glasses','handbag','tie','suitcase','frisbee','skis','snowboard','sports ball','kite','baseball bat','baseball glove','skateboard',\n",
        "'surfboard','tennis racket','bottle','plate','wine glass','cup','fork','knife','spoon','bowl','banana','apple','sandwich','orange','broccoli','carrot','hot dog','pizza','donut','cake','chair','couch','potted plant','bed',\n",
        "'mirror','dining table','window','desk','toilet','door','tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven','toaster','sink','refrigerator','blender','book','clock','vase','scissors','teddy bear',\n",
        "'hair drier','toothbrush','hair brush'\n",
        "]\n",
        "\n",
        "class_name_to_idx = {elem:idx for idx, elem in enumerate(class_names)}\n",
        "n_classes = len(class_names)\n",
        "co_occurrence_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "with fo.ProgressBar() as pb:\n",
        "  for sample in pb(dataset):\n",
        "    detections = sample.ground_truth.detections\n",
        "    for attention_pair in product(detections, detections):\n",
        "      i = class_name_to_idx[attention_pair[0].label]\n",
        "      j = class_name_to_idx[attention_pair[1].label]\n",
        "      co_occurrence_matrix[i][j] += 1\n",
        "      co_occurrence_matrix[j][i] += 1"
      ],
      "metadata": {
        "id": "rpwK7r_HmWjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_matrix = co_occurrence_matrix / np.amax(co_occurrence_matrix + 1.0, axis=1)\n",
        "\n",
        "figure(figsize=(20, 20), dpi=80)\n",
        "ax = sns.heatmap(normalized_matrix, linewidth=0.5, xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fA_VGr0roL5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining PyTorch dataset"
      ],
      "metadata": {
        "id": "pmsMpYrnrDUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "_OC7C5JevCgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Resize(object):\n",
        "  def __init__(self, size, interpolation=TVF.InterpolationMode.BILINEAR, max_size=None, antialias=None):\n",
        "    self.size = size\n",
        "    self.max_size = max_size\n",
        "    self.interpolation = interpolation\n",
        "    self.antialias = antialias\n",
        "\n",
        "  def __call__(self, image, target):\n",
        "    assert torch.is_tensor(image), \"Image is expected to be of type torch.tensor\"\n",
        "    original_shape = image.shape\n",
        "    image = TVF.resize(image, self.size, self.interpolation, self.max_size, self.antialias)\n",
        "    transformed_shape = image.shape\n",
        "    x_scale = transformed_shape[2] / original_shape[2]\n",
        "    y_scale = transformed_shape[1] / original_shape[1]\n",
        "    scale_tensor = torch.tensor([x_scale, y_scale]).repeat(target['boxes'].shape[0], 2)\n",
        "    target['boxes'] = torch.mul(target['boxes'], scale_tensor)\n",
        "    return image, target"
      ],
      "metadata": {
        "id": "0f4N0hFYrmUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchCocoDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, fiftyone_dataset, transforms=None, gt_field=\"ground_truth\"):\n",
        "    self.samples = fiftyone_dataset\n",
        "    self.transforms = transforms\n",
        "    self.gt_field = gt_field\n",
        "    self.img_paths = self.samples.values(\"filepath\")\n",
        "    self.classes = class_names\n",
        "    self.labels_map_rev = class_name_to_idx\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.img_paths[idx]\n",
        "    sample = self.samples[img_path]\n",
        "    metadata = sample.metadata\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    area = []\n",
        "    iscrowd = []\n",
        "    detections = sample[self.gt_field].detections\n",
        "\n",
        "    for det in detections:\n",
        "      category_id = self.labels_map_rev[det.label]\n",
        "      coco_obj = fouc.COCOObject.from_label(\n",
        "          det, metadata, category_id=category_id,\n",
        "      )\n",
        "      x, y, w, h = coco_obj.bbox\n",
        "      boxes.append([x, y, x + w, y + h])\n",
        "      labels.append(coco_obj.category_id)\n",
        "      area.append(coco_obj.area)\n",
        "      iscrowd.append(coco_obj.iscrowd)\n",
        "\n",
        "    target = {}\n",
        "    target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "    target[\"image_id\"] = torch.as_tensor([idx])\n",
        "    target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n",
        "    target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "    if self.transforms:\n",
        "        img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "\n",
        "  def get_classes(self):\n",
        "    return self.classes"
      ],
      "metadata": {
        "id": "OhBWtDFwqdEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "has_bounding_box_view = dataset.filter_labels(\n",
        "        \"ground_truth\",\n",
        "        VF(\"label\").is_in(class_names)\n",
        ")\n",
        "\n",
        "train_transforms = Compose([ToTensor(), Resize(size=(512, 512))])\n",
        "test_transforms = Compose([ToTensor()])\n",
        "\n",
        "train_view = has_bounding_box_view.take(150, seed=51)\n",
        "test_view = has_bounding_box_view.exclude([s.id for s in train_view])\n",
        "\n",
        "print(f\"Number of training samples: {len(train_view)}\")\n",
        "print(f\"Number of test samples: {len(test_view)}\")\n",
        "\n",
        "torch_dataset_train = TorchCocoDataset(train_view, train_transforms)\n",
        "torch_dataset_test = TorchCocoDataset(test_view, test_transforms)"
      ],
      "metadata": {
        "id": "wKLyHQYlt-H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_image_with_bounding_boxes(image, target):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(image.permute(1, 2, 0))\n",
        "  for box in target['boxes']:\n",
        "    x = box[0]\n",
        "    y = box[1]\n",
        "    h = box[2] - x\n",
        "    w = box[3] - y\n",
        "    rect = patches.Rectangle((x, y), h, w, linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-YHMZA5MuYh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_image_with_bounding_boxes(*torch_dataset_train[0])"
      ],
      "metadata": {
        "id": "Bw-7xlh51xBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model"
      ],
      "metadata": {
        "id": "RTJ22qC9xJXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATorGraph(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def __call__(self, x, batched_boxes):\n",
        "    # print(f\"GATorGraph: x shape before: {x.shape}\")\n",
        "    edges = []\n",
        "    for index, boxes in enumerate(batched_boxes):\n",
        "      ious = box_iou(boxes, boxes)\n",
        "      ious = torch.triu(ious, diagonal=1)\n",
        "      edge_list = (ious > 0.5).nonzero(as_tuple=False)\n",
        "      # print(f\"GATorGraph: x shape in for loop: {x[index].shape}\") \n",
        "      edges.append(pyg_data.Data(x=x[index], edge_index=edge_list.T))\n",
        "\n",
        "    batched_edge_lists = pyg_data.Batch.from_data_list(edges)\n",
        "    # print(f\"GATorGraph: x shape after: {x.shape}\")\n",
        "    # print(f\"GATorGraph: data num nodes: {edges[0]}\")\n",
        "    return batched_edge_lists"
      ],
      "metadata": {
        "id": "kftiJY9CICT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATor(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, layers, heads, dropout, edge_dim=None):\n",
        "    super(GATor, self).__init__()\n",
        "    self.convs = nn.ModuleList()\n",
        "    self.convs.append(GATConv(in_channels=input_dim, out_channels=hidden_dim, heads=heads, edge_dim=edge_dim))\n",
        "    for l in range(layers-1):\n",
        "      self.convs.append(GATConv(in_channels=heads * hidden_dim, out_channels=hidden_dim, heads=heads, edge_dim=edge_dim))\n",
        "    # post-message-passing\n",
        "    self.post_mp = nn.Sequential(\n",
        "      nn.Linear(heads * hidden_dim, hidden_dim), nn.Dropout(dropout), \n",
        "      nn.Linear(hidden_dim, output_dim))\n",
        "    self.dropout = dropout\n",
        "    self.num_layers = layers\n",
        "\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "    # print(f\"GATor x shape: {x.shape}\")  \n",
        "    # print(f\"GATor edge_index shape: {edge_index.shape}\")\n",
        "    for i in range(self.num_layers):\n",
        "      # print(f\"GATor current layer index: {i}\")\n",
        "      # print(f\"GATor current x shape: {x.shape}\")\n",
        "      x = self.convs[i](x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "    x = self.post_mp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "F5ViSKT680Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiScaleRoIAlignWrapper(nn.Module):\n",
        "  def __init__(self, roi_align):\n",
        "    super(MultiScaleRoIAlignWrapper, self).__init__()\n",
        "    self.roi_align = roi_align\n",
        "\n",
        "  def forward(self, x, boxes, image_shapes):\n",
        "    # print(f\"MULTI SCALE ROI: X dictionary: {x}\")\n",
        "    # print(f\"MULTI SCALE ROI: boxes len: {len(boxes)}\")\n",
        "    # print(f\"MULTI SCALE ROI: boxe[0] shape: {boxes[0].shape}\")\n",
        "    # print(f\"MULTI SCALE ROI: image_shapes len: {len(image_shapes)}\")\n",
        "    # for a in x:\n",
        "    #   print(f\"MULTI SCALE ROI: shapes o elements in X: {a}\")\n",
        "    ret_val = self.roi_align(x, boxes, image_shapes)\n",
        "    # print(f\"MULTI SCALE ROI: retval: {ret_val}\")\n",
        "    # print(f\"MULTI SCALE ROI: retval shape: {ret_val.shape}\")\n",
        "    cached['boxes'] = boxes\n",
        "    return ret_val"
      ],
      "metadata": {
        "id": "PNU5fMCOBbbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoMLPHeadWrapper(nn.Module):\n",
        "  def __init__(self, box_head, gator_graph, gator):\n",
        "    super(TwoMLPHeadWrapper, self).__init__()\n",
        "    self.box_head = box_head\n",
        "    self.gator_graph = gator_graph\n",
        "    self.gator = gator\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(f\"TwoMLPHeadWrapper: x shape: {x.shape}\")\n",
        "    node_features = self.box_head(x)\n",
        "    # print(f\"TwoMLPHeadWrapper: node_features shape: {node_features.shape}\")\n",
        "    B = len(cached['boxes'])\n",
        "    graph = self.gator_graph(node_features.view(B,-1, node_features.shape[1]), cached['boxes'])\n",
        "    ret_val = self.gator(graph)\n",
        "    # print(f\"TwoMLPHeadWrapper: Evo me\")\n",
        "    return ret_val"
      ],
      "metadata": {
        "id": "6sm1fgPfFTsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cached = {}\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "gator_graph = GATorGraph()\n",
        "gator = GATor(input_dim=1024, hidden_dim=1024, output_dim=1024, layers=2, heads=8, dropout=0.5, edge_dim=None)\n",
        "# gat = GATModule(model.roi_heads.box_head, gator_graph)\n",
        "# model.rpn = RPNWrapper(model.rpn)\n",
        "model.roi_heads.box_roi_pool = MultiScaleRoIAlignWrapper(model.roi_heads.box_roi_pool)\n",
        "model.roi_heads.box_head = TwoMLPHeadWrapper(model.roi_heads.box_head, gator_graph, gator)\n",
        "# model.box_predictor.reset_parameters()\n",
        "# model.roi_heads.box_head = gat\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "kNaBSo_AxikF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining PyTorch dataloaders"
      ],
      "metadata": {
        "id": "-JLQATBxx1ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))"
      ],
      "metadata": {
        "id": "cspE-mQk1MS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader_train = DataLoader(torch_dataset_train, batch_size=4, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "data_loader_test = DataLoader(torch_dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "z5ONelLMx9s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "oSHErrODx4_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, n_epochs):\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  model.train()\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "  for param in model.roi_heads.box_head.parameters():\n",
        "    param.requires_grad = True\n",
        "  for param in model.roi_heads.box_predictor.parameters():\n",
        "    param.requires_grad = True\n",
        "  model.to(device)\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, test_loader, device=device)"
      ],
      "metadata": {
        "id": "yY3YPOCYyOgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, data_loader_train, data_loader_test, n_epochs=2)"
      ],
      "metadata": {
        "id": "lkjjcTSwyiBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gZZMDUs8x61X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_torch_predictions(preds, det_id, s_id, w, h, classes):\n",
        "  # Convert the outputs of the torch model into a FiftyOne Detections object\n",
        "  dets = []\n",
        "  for bbox, label, score in zip(preds[\"boxes\"].cpu().detach().numpy(), preds[\"labels\"].cpu().detach().numpy(), preds[\"scores\"].cpu().detach().numpy()):\n",
        "    # Parse prediction into FiftyOne Detection object\n",
        "    x0,y0,x1,y1 = bbox\n",
        "    coco_obj = fouc.COCOObject(det_id, s_id, int(label), [x0, y0, x1-x0, y1-y0])\n",
        "    det = coco_obj.to_detection((w,h), classes)\n",
        "    det[\"confidence\"] = float(score)\n",
        "    dets.append(det)\n",
        "    det_id += 1\n",
        "        \n",
        "  detections = fo.Detections(detections=dets)\n",
        "      \n",
        "  return detections, det_id"
      ],
      "metadata": {
        "id": "BWxldv0uxqtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_detections(model, torch_dataset, view, field_name=\"predictions\"):\n",
        "  # Run inference on a dataset and add results to FiftyOne\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  image_paths = torch_dataset.img_paths\n",
        "  classes = torch_dataset.classes\n",
        "  det_id = 0\n",
        "  \n",
        "  print(\"Adding detections to FiftyOne view...\")\n",
        "  with fo.ProgressBar() as pb:\n",
        "    for img, targets in pb(torch_dataset):\n",
        "      # Get FiftyOne sample indexed by unique image filepath\n",
        "      img_id = int(targets[\"image_id\"][0])\n",
        "      img_path = image_paths[img_id]\n",
        "      sample = view[img_path]\n",
        "      s_id = sample.id\n",
        "      w = sample.metadata[\"width\"]\n",
        "      h = sample.metadata[\"height\"]\n",
        "      \n",
        "      # Inference\n",
        "      preds = model(img.unsqueeze(0).to(device))[0]\n",
        "      \n",
        "      detections, det_id = convert_torch_predictions(preds, det_id, s_id, w, h, classes)\n",
        "      \n",
        "      sample[field_name] = detections\n",
        "      sample.save()"
      ],
      "metadata": {
        "id": "aNcCqXr93zrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_detections(model, torch_dataset_test, test_view)\n",
        "\n",
        "results = fo.evaluate_detections(test_view, \"predictions\", eval_key=\"eval\", compute_mAP=True) "
      ],
      "metadata": {
        "id": "yk0MYvJG4AQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.mAP())"
      ],
      "metadata": {
        "id": "XU39GIkR429G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.plot_confusion_matrix()"
      ],
      "metadata": {
        "id": "PVc1EfhA431e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.plot_pr_curves()"
      ],
      "metadata": {
        "id": "8w4iYu0H451L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session.view = test_view"
      ],
      "metadata": {
        "id": "PpWYNI1T4Gdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XQZOlMz464w3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}